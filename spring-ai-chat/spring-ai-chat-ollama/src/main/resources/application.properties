spring.application.name=spring-ai-chat-ollama
server.port=8081
spring.profiles.active=ollama

# Ollama configuration for Spring AI Chat
# The address of the Ollama service, which defaults to http://localhost:11434.
spring.ai.ollama.base-url=http://localhost:11434
# The model name which is the one you previously downloaded via the terminal using `ollama pull` or `ollama run`
#spring.ai.ollama.chat.model=deepseek-r1:1.5b
spring.ai.ollama.chat.model=qwen2.5:7b
spring.ai.ollama.chat.options.temperature=0.7